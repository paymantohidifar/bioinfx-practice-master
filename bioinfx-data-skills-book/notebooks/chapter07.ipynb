{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e63aeb94-74ec-4650-ac2b-c93ddedb2dd3",
   "metadata": {},
   "source": [
    "# Chapter 7: Unix Data Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e235e4a-7f54-49a3-b33e-8ecc84918a65",
   "metadata": {},
   "source": [
    "### Unix Data Tools and the Unix One-Liner Approach: Lessons from Programming Pearls\n",
    "\n",
    "The speed and power of this approach is why it’s a core part of bioinformatics work. For example, the following one-liner returns $K$ most common work in a file:\n",
    "\n",
    "```bash\n",
    "cat input.txt | tr -cs A-Za-z '\\n' | tr A-Z a-z | sort | uniq -c | sort -rn | sed 10q\n",
    "```\n",
    "\n",
    "### When to Use the Unix Pipeline Approach and How to Use It Safely\n",
    "\n",
    "Fast, low-level data manipulation toolkit to:\n",
    "- Explore data\n",
    "- Transform data between formats\n",
    "- Inspect data for potential problems\n",
    "\n",
    "For larger, more complex tasks it’s often preferable to write a custom script in a language like Python (or R if the work involves lots of data analysis).\n",
    "\n",
    "Lengthy Unix pipelines can be fragile and less robust than a custom script.\n",
    "\n",
    "Storing pipelines in scripts is a good approach — not only do scripts serve as documentation of what steps were performed\n",
    "on data, but they allow pipelines to be rerun and can be checked into a Git repository.\n",
    "\n",
    "### Inspecting and Manipulating Text Data with Unix Tools\n",
    "\n",
    "There are mainly three types of delimited files:\n",
    "1. Tab-delimited (TSV)\n",
    "2. Comma-separated (CSV)\n",
    "3. Variable space-delimited (VSV)\n",
    "\n",
    "Of these three formats, tab-delimited is the most commonly used in bioinformatics. File formats such as BED, GTF/GFF, SAM, tabular BLAST output, and VCF are all examples of tab-delimited files.\n",
    "\n",
    "Sometimes it’s useful to see both the beginning and end of a file:\n",
    "\n",
    "```bash\n",
    "# Return first and last two lines\n",
    "(head -n2; tail -n2) < file.txt\n",
    "```\n",
    "#### **less (zless)**\n",
    "\n",
    "Common uses of `less` or `zless`:\n",
    "- Allows you to search text and highlights matches.\n",
    "- Helps with debugging our command-line pipelines.\n",
    "- Crucial when iteratively building up a pipeline — which is the best way to construct pipelines.\n",
    "  \n",
    "  ```bash\n",
    "  step1 input.txt | less                  # inspect output in less\n",
    "  step1 input.txt | step2 | less\n",
    "  step1 input.txt | step2 | step3 | less\n",
    "  ```\n",
    "- Piping a program's output to `less` pauses the program's execution when the screen is full.\n",
    "- This happens because `less` stops reading data from the pipe once its buffer is full, causing the pipe to become blocked.\n",
    "- The blocked pipe then prevents the sending program from writing more data, effectively pausing its process.\n",
    "- This behavior allows you to safely inspect the output of a long-running command without it wasting CPU resources to process data you aren't yet viewing.\n",
    "\n",
    "Here is commonly used `less` commands:\n",
    "\n",
    "| Shortcut  | Action    |\n",
    "|---------- | ----------|\n",
    "| space bar | Next page |\n",
    "| b         | Provioubs page |\n",
    "| g         | First line |\n",
    "| G         | Last line  |\n",
    "| j         | Down (one line at a time) |\n",
    "| k         | Up (one line at a time)   |\n",
    "| /\\<pattern\\>| Search down (forward) for string \\<pattern\\> |\n",
    "| ?\\<pattern\\>| Search up (backword) for string \\<pattern\\> |\n",
    "| n          | Repeat last search downword (forward) |\n",
    "| N          | Repeat last search upward (backword)  |\n",
    "\n",
    "\n",
    "#### **cut** and **column**\n",
    "\n",
    "```bash\n",
    "# cut 3rd column from tab-delimited file\n",
    "cut -f 3 file.tsv \n",
    "\n",
    "# cut 3rd-5th columns from tab-delimited file\n",
    "cut -f 3-5 file.tsv\n",
    "\n",
    "#cut 3rd, 5th, 7th columns from tab-delimited file\n",
    "cut -f 3,5,7 file.tsv\n",
    "\n",
    "# Combined selection of columns\n",
    "cut -f1-4,6,7 file.tsv\n",
    "\n",
    "# Cut columns from csv file\n",
    "cut -d, -f 2 file.csv\n",
    "```\n",
    "\n",
    "Note that usig `cut`, we cannot reorder columns. To do that we will use `awk`.\n",
    "\n",
    "Program `column -t` (the `-t` option tells column to treat data as a table) produces neat columns that are much easier to read. Like `cut`, `column`’s default delimiter is the tab character (`\\t`). We can specify a different delimiter with the `-s` option.\n",
    "\n",
    "Note that you should only use columnt `-t` to visualize data in the terminal, not to reformat data to write to a file.\n",
    "\n",
    "#### **grep**\n",
    "\n",
    "- Fastest program to find patterns (fixed string or regular expression) in a file:\n",
    "\n",
    "<img src='images/grep_speed.png' width='600'>\n",
    "\n",
    "We can show context around any match using:\n",
    "- `-A`: after match\n",
    "- `-B`: before match\n",
    "- `-C`: after and before\n",
    "\n",
    "```bash\n",
    "# Return two lines after matched <pattern> line\n",
    "grep -A2 \"<pattern>\" file\n",
    "```\n",
    "\n",
    "**grep's regular expression:**\n",
    "- supports a flavor of regular expression called POSIX Basic Regular Expressions (BRE)\n",
    "- Not as strong and comprehnsive as Python's or Perl's\n",
    "- For more complex RE's we can activate its POSIX Extended Regular Expressions (ERE) using `-E` option.\n",
    "\n",
    "**grep's other useful options:**\n",
    "- `-c`: counts matching lines\n",
    "- `-o`: outputs only matching part of the pattern\n",
    "- `-w`: matchs the whole word separated with white space\n",
    "\n",
    "#### **sort**\n",
    "- `sort` is designed to work with plain-text data with columns.\n",
    "- Running `sort` without any arguments simply sorts a file alphanumerically by line.\n",
    "- By default, `sort` treats blank characters (like tab or spaces) as field delimiters. If your file uses another delimiter (such as a comma for CSV files), you can specify the field separator with `-t` (e.g., `-t\",\"`).\n",
    "- To specify a sorting key, we use `-k` arguments. Each `-k` argument takes a range of columns as `start,end`. So to sort by a single column we use `start,start`. For example, to sort a file using first column and second column (a number), we can use the following command:\n",
    "    ```bash\n",
    "    sort -k1,1 -k2,2n file.bed\n",
    "    ```\n",
    "- If we don’t want sort to change the order of lines that are equal according to our sort keys, we can specify the `-s` option. `-s` turns off this last-resort sorting, thus making sort a stable sorting algorithm.\n",
    "- Sorting can be computationally intensive. We can check if a file is sorted according to our `-k` arguments using `-c`. If file is already sorted, `echo $?` should return `0`, otherwise, it should return `1`.\n",
    "    ```bash\n",
    "    sort -k1,1 -k2,2n -c file_sorted.bed\n",
    "    ```\n",
    "- We can sort in reverse order using `-r` option:\n",
    "\n",
    "    ```bash\n",
    "    # Reverse based on first column\n",
    "    sort -k1,1 -k2,2n -r file.bed\n",
    "    # Reverse based on second column\n",
    "    sort -k1,1 -k2,2nr file.bed\n",
    "- For clever sorting, specially whe sorting alphanumerically, we should use `-V` in the sorting key:\n",
    "\n",
    "    ```bash\n",
    "    sort -k1,1V file.bed\n",
    "    ```\n",
    "- Under the hood, `sort` uses a fixed-sized memory buffer to sort as much data in-memory as fits. Increasing the size of this buffer allows more data to be sorted in memory, which reduces the amount of temporary sorted files that need to be written and read off the disk. To do this, we can use `-S` argument. The `-S` argument understands suffixes like **K** for kilobyte, **M** for megabyte, and **G** for gigabyte, as well as **%** for specifying what percent of total memory to use (e.g., 50% with `-S 50%`). For example:\n",
    "\n",
    "    ```bash\n",
    "    # Allocate 2G to sort on memory\n",
    "    sort -k1,1 -k2,2n -S2G file.bed\n",
    "    ```\n",
    "- We can also run `sort` on multiple cores (parallel runs) with `--parallel` argument:\n",
    "\n",
    "    ```bash\n",
    "    # Run sort on 4 cores\n",
    "    sort -k1,1 -k2,2n --parallel 4 file.bed\n",
    "    ```\n",
    "\n",
    "#### **uniq**\n",
    "\n",
    "Unix’s `uniq` takes lines from a file or standard input stream, and outputs all lines with **consecutive duplicates removed**. If we want to find all unique lines in a file, we would first sort all lines using `sort` so that all identical lines are grouped next to each other, and then run `uniq`.\n",
    "\n",
    "**Uniq oprions:**\n",
    "- `-i`: makes uniq to be case-insensitive\n",
    "- `-c`: shows the counts of occurrences next to the unique lines.\n",
    "- `-d`: checks for duplicates\n",
    "\n",
    "#### **join**\n",
    "- The Unix tool `join` is used to join different files together by a common column.\n",
    "- To append the two files on their commom column, we first need to sort both files by the column to be joined on. This is a vital step—Unix’s join will not work unless both files are sorted by the column to join on.\n",
    "- The basic syntax is `join -1 <file_1_field> -2 <file_2_field> <file_1> <file_2>`, where `<file_1>` and `<file_2>` are the two files to be joined by a column `<file_1_field>` in `<file_1>` and column `<file_2_field>` in `<file_2>`.\n",
    "- GNU `join` implements the `-a` option to include unpairable lines — ones that do not have an entry in either file. To use `-a`, we specify which file is allowed to have unpairable entries:\n",
    "\n",
    "```bash\n",
    "join -1 1 -2 1 -a 1 file1.txt file2.txt\n",
    "```\n",
    "\n",
    "\n",
    "#### **awk**\n",
    "- `Awk` is an easy, small programming language great at working with text data like TSV and CSV files.\n",
    "- The key to using `Awk` effectively is to reserve it for the subset of tasks it’s best at: quick data-processing tasks on tabular data.\n",
    "- There’s also `GNU Awk`, known as `Gawk`, which is based on the original Awk but has many extended features.\n",
    "\n",
    "**How AwK works:**\n",
    "\n",
    "* **Data Structure**: Awk processes data one record at a time. Each **record** is a line, and each **field** is a column within that line.\n",
    "* **Automatic Variables**: Awk automatically assigns the entire record to the variable `$0` and each field to a corresponding variable, such as `$1`, `$2`, and so on.\n",
    "* **Program Structure**: Awk programs are built using `pattern { action }` pairs.\n",
    "    * The `pattern` is an expression or regular expression that, if true, triggers the `action`.\n",
    "    * The `action` contains the commands to be executed.\n",
    "* **Optional Components**: You can omit either the pattern or the action.\n",
    "    * If you omit the pattern, the action runs on all records.\n",
    "    * If you omit the action, Awk prints all records that match the pattern.\n",
    "* **Summary**: These two core concepts—records/fields and pattern-action pairs—form the foundation of using Awk for text processing.\n",
    "\n",
    "For example, the following awk command mimmics GNU `cat`:\n",
    "\n",
    "```bash\n",
    "awk '{print $0}' file.tsv\n",
    "```\n",
    "In above command, `print` prints a string.\n",
    "\n",
    "- Awk supports arithmetic with the standard operators +, -, *, /, % (remainder), and ^ (exponentiation).\n",
    "\n",
    "**Awk comparison and logical operators**\n",
    "\n",
    "<img src=\"images/awk_comp_logic.png\" width=\"400\">\n",
    "\n",
    "We can also chain patterns, by using logical operators `&&` (AND), `||` (OR), and `!` (NOT). For example, if we wanted all lines on chromosome 1 with a length greater than 10:\n",
    "\n",
    "```bash\n",
    "$ awk '$1 ~ /chr1/ && $3 - $2 > 10' example.bed\n",
    "# [output]\n",
    "# chr1 26 39\n",
    "# chr1 32 47\n",
    "# chr1 9  28\n",
    "```\n",
    "\n",
    "The first pattern, `$1 ~ /chr1/`, is how we specify a regular expression. Regular expressions are in **slashes**.\n",
    "\n",
    "We can combine patterns and more complex actions than just printing the entire record. For example, if we wanted to add a column with the length of this feature (end position - start position) for only chromosomes 2 and 3, we could use:\n",
    "\n",
    "```bash\n",
    "$ awk '$1 ~ /chr2|chr3/ { print $0 \"\\t\" $3 - $2 }' example.bed\n",
    "# [Output]\n",
    "# chr3 11 28 17\n",
    "# chr3 16 27 11\n",
    "# chr2 35 54 19\n",
    "```\n",
    "\n",
    "**`BEGIN` and `END` patterns:**\n",
    "- The `BEGIN` pattern specifies what to do *before* the first record is read in. It is useful to initialize and set up variables.\n",
    "- `END` specifies what to do *after* the last record’s processing is complete. It is useful to print data summaries at the end of file processing.\n",
    "\n",
    "For example, suppose we wanted to calculate the mean feature length. We would have to take the sum \n",
    "feature lengths, and then divide by the total number of records. We can do this with:\n",
    "\n",
    "```bash\n",
    "$ awk 'BEGIN{ s = 0 }; { s += ($3-$2) }; END{ print \"mean: \" s/NR };' example.bed\n",
    "# [Output]\n",
    "# mean: 14\n",
    "```\n",
    "In above example, `NR` is the current record number, so on the last record `NR` is set to the total number of records processed. In this example, we’ve initialized a variable `s` to 0 in `BEGIN` (variables you define do not need a dollar sign). Then, for each record we increment `s` by the length of the feature. At the end of the records, we print this sum `s` divided by the number of records `NR`, giving the mean.\n",
    "\n",
    "- We can use `NR` to extract ranges of lines. For example, if we wanted to extract all lines between 3 and 5 (inclusive):\n",
    "\n",
    "    ```bash\n",
    "    awk 'NR >=3 && NR <=5' example.bed\n",
    "    ```\n",
    "\n",
    "- AWK's **associative arrays** are a powerful data structure that works like a **dictionary** in Python or a hash map in other languages. Instead of using a numeric index, you access and store data using a **key** and a **value**. This is useful for tasks such as counting items, like features belonging to a specific gene, by simply assigning a value to a key.\n",
    "\n",
    "    ```bash\n",
    "    awk '/gene_name/ {feat[$3] += 1}; END {for (i in feat) print i \"\\t\" feat[i] }' example.gtf \n",
    "    ```\n",
    "    \n",
    "- Awk has several useful built-in functions:\n",
    "\n",
    "<img src=\"images/awk_funcs.png\" width=\"400\">\n",
    "\n",
    "\n",
    "**Setting Field, Output Field, and Record Separators:**\n",
    "\n",
    "* **Field Separator (-F):** You can use the `-F` flag to specify a field separator other than whitespace, like a comma for a CSV file (e.g., `awk -F\",\"`).\n",
    "* **Variable Assignments (-v):** The `-v` flag allows you to set variables. This is useful for specifying other separators.\n",
    "* **Output Field Separator (OFS):** The `OFS` variable defines the character that separates fields in the output. For example, `awk -F\",\" -v OFS=\"\\t\"` converts a comma-separated file to a tab-separated one.\n",
    "* **Other Separators:** You can also set the Record Separator (`RS`) and Output Record Separator (`ORS`).\n",
    "\n",
    "#### Bioawk: An Awk for Biological Formats \n",
    "\n",
    "Bioawk is the extension of Awk, developed by Heng Li, that is tailored for bioinformatics. The basic idea of Bioawk is that we specify what bioinformatics format we’re working with, and Bioawk will automatically set variables for each field (just as regular Awk sets the columns of a tabular text file to `$1`, `$1`, `$2`, etc.). For Bioawk to set these fields, specify the format of the input file or stream with `-c`.\n",
    "\n",
    "```bash\n",
    "bioawk -c help\n",
    "```\n",
    "\n",
    "Bioawk is also quite useful for processing FASTA/FASTQ files. For example, we could use it to turn a FASTQ file into a FASTA file:\n",
    "\n",
    "```bash\n",
    "bioawk -c fastx '{print \">\"$name\"\\n\"$seq}' contam.fastq\n",
    "```\n",
    "\n",
    "Or Bioawk’s function `revcomp()` can be used to reverse complement a sequence:\n",
    "\n",
    "```bash\n",
    "$ bioawk -c fastx '{print \">\"$name\"\\n\"revcomp($seq)}' contam.fastq\n",
    "```\n",
    "\n",
    "Bioawk offers two convenient options for working with tab-delimited files:\n",
    "\n",
    "* **-t:** This option is for general tab-delimited files. It automatically sets both the input and output field separators to tabs, which saves you from having to manually set `FS` and `OFS`.\n",
    "* **-c hdr:** Use this option for tab-delimited files that have a header on the first line. It not only sets the field separators to tabs but also uses the names from the header row to create variables, making it easier to refer to specific columns.\n",
    "\n",
    "For example, for the following file:\n",
    "\n",
    "```bash\n",
    "$ head -n 4 genotypes.txt\n",
    "# [Output]\n",
    "# id    ind_A ind_B ind_C\n",
    "# S_000 T/T   A/T   A/T\n",
    "# S_001 G/C   C/C   C/C\n",
    "# S_002 C/A   A/C   C/C\n",
    "```\n",
    "If we wanted to return all variants for which individuals `ind_A` and `ind_B` have identical genotypes:\n",
    "\n",
    "```bash\n",
    "bioawk -c hdr '$ind_A == $ind_B {print $id}' genotypes.txt\n",
    "```\n",
    "\n",
    "#### Stream Editing with Sed\n",
    "- The stream editor, or `sed`, allows you to make trivial edits to a stream, usually to prepare it for the next step in a Unix pipeline.\n",
    "- `sed` reads data from a file or standard input and can edit **a line at a time** and allow us to edit it without opening the entire file in memory (very useful for large files).\n",
    "\n",
    "For example, we want substitute a `chrom` to `chr` in each line `chroms.txt` file:\n",
    "\n",
    "```bash\n",
    "sed 's/chrom/chr/' chroms.txt > modified_chroms.txt\n",
    "```\n",
    "\n",
    "- The syntax of sed’s substitute is `s/pattern/replacement/`. By default, `sed` only replaces the first occurrence of a match. To replace all occurrences of strings that match our pattern, we need to set the global flag `g` after the last slash: `s/pattern/replacement/g`. For case-insensitive matching, we can enable this with the flag `i` (e.g., `s/pattern/replacement/i`).\n",
    "- `-n`: disables `sed` from outputting all lines.\n",
    "- It’s also possible to select and print certain ranges of lines with sed. To print the first 10 lines of a\n",
    "file (similar to `head -n 10`), we use:\n",
    "\n",
    "    ```bash\n",
    "    sed -n '1,10p' Mus_musculus.GRCm38.75_chr1.gtf\n",
    "    ```\n",
    "    If we wanted to print lines 20 through 50, we would use:\n",
    "\n",
    "    ```bash\n",
    "    sed -n '20,50p' Mus_musculus.GRCm38.75_chr1.gtf\n",
    "    ```\n",
    "- By appending `p` after the last number, `sed` will print all lines requested.\n",
    "   \n",
    "#### Decoding Plain-Text Data: hexdump\n",
    "\n",
    "* **ASCII Encoding**: ASCII is a character encoding scheme that uses 7 bits to represent 128 different characters, including letters, numbers, and symbols.\n",
    "* **Modern Computers and ASCII**: Although ASCII only uses 7 bits, modern computers typically use an 8-bit byte to store ASCII characters.\n",
    "* **Relevance in Bioinformatics**: Plain-text data in bioinformatics is often encoded in ASCII.\n",
    "* **When Encoding Matters**: While most of the time you won't need to worry about encoding details, issues can arise when non-ASCII or invisible characters are present in a file, causing errors and \"major headaches.\"\n",
    "* **Non-ASCII formats**: May contain special characters that cause may cause problems with bioinformatic tools. The most common character encoding scheme is UTF-8, which is a superset of ASCII but allows for special characters.\n",
    "* **Identify file format**: Use `file` command to infer what the encoding is from the file's content.\n",
    "* **Decipher non-ASCII characters**: Use `hexdump` to return hexadecimal values of each character: `hexdump -c file.txt`.\n",
    "\n",
    "\n",
    "### Subshells\n",
    "\n",
    "**Sequential vs. Piped Commands**\n",
    "\n",
    "* **Sequential Commands:** These commands run one after the other. The output from the first command does not automatically become the input for the next command.\n",
    "    * If we run two commands with `command1 ; command2`, `command2` will always run, regardless of whether command1 exits successfully (with a zero exit status). In contrast, if we use `command1 && command2`, `command2` will only run if `command1` completed with a zero-exit status.\n",
    "* **Piped Commands:** When you use a pipe (`|`) to connect two commands, the standard output of the first command is sent directly to the standard input of the second command. This lets you chain multiple commands together to process data.\n",
    "\n",
    "You can group multiple sequential commands with parentheses `()` so that their combined output is treated as a single stream. This single stream can then be piped (`|`) as input to another command.\n",
    "\n",
    "```bash\n",
    "echo \"this command\"; echo \"that command\" | sed 's/command/step/'\n",
    "# [Output]\n",
    "# this command\n",
    "# that step\n",
    "(echo \"this command\"; echo \"that command\") | sed 's/command/step/'\n",
    "# [Output]\n",
    "# this step\n",
    "# that step\n",
    "```\n",
    "\n",
    "Another example of using subshells:\n",
    "\n",
    "```bash\n",
    "(zgrep \"^#\" Mus_musculus.GRCm38.75_chr1.gtf.gz; \\\n",
    "zgrep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf.gz | \\\n",
    "sort -k1,1 -k4,4n) | gzip > Mus_musculus.GRCm38.75_chr1_sorted.gtf.gz\n",
    "```\n",
    "\n",
    "### Named Pipes and Process Substitution\n",
    "\n",
    "Named pipes are a solution for programs that cannot use standard Unix pipes. Here are the key takeaways:\n",
    "\n",
    "* **The Problem:** Some programs, especially in bioinformatics, require separate input files and produce separate output files, making it impossible to use the standard Unix pipe (`|`) to connect them in a pipeline.\n",
    "* **The Bottleneck:** This limitation forces the user to write and read multiple temporary files to and from the disk, which is a very slow process and creates a significant performance bottleneck in a data-processing pipeline.\n",
    "* **The Solution:** A **named pipe** (also known as a **FIFO**, First In First Out) is a special type of file that acts like a pipe but has a name and exists on the filesystem. It allows you to pass data between programs that require explicit file inputs and outputs, avoiding the need to write to disk and thus maintaining the speed of a pipeline.\n",
    "* We can create a named pipe with the program `mkfifo`.\n",
    "* Like a file, we clean up by using `rm` to remove it.\n",
    "\n",
    "```bash\n",
    "mkfifo fqin\n",
    "ls -l fqin\n",
    "# [Output]\n",
    "# prw-r--r--   1 vinceb  staff    0 Aug 5 22:50 fqin\n",
    "\n",
    "echo \"hello, named pipes\" > fqin &\n",
    "cat fqin\n",
    "rm fqin\n",
    "```\n",
    "\n",
    "You’ll notice that this is indeed a special type of file: the `p` before the file permissions is for pipe.\n",
    "\n",
    "Process substitution are ways to make data pipelines more flexible:\n",
    "\n",
    "* **Process Substitution:** This is a shortcut that creates an anonymous named pipe on the fly. It allows the output of a command to be treated as a file, which can then be used as input for a program. This removes the need to manually create and remove named pipe files, making it easier to connect programs that don't accept standard input.\n",
    "* **The Difference to named pipes:** While both achieve similar goals, a named pipe is a persistent file you create with `mkfifo`, whereas process substitution creates a temporary, unnamed pipe that the shell handles automatically, making it ideal for quick command-line use.\n",
    "\n",
    "Here are two examples:\n",
    "\n",
    "```bash\n",
    "program --in1 <(makein raw1.txt) --in2 <(makein raw2.txt) \\\n",
    "--out1 out1.txt --out2 out2.txt\n",
    "```\n",
    "\n",
    "```bash\n",
    "program --in1 in1.txt --in2 in2.txt \\\n",
    "--out1 >(gzip > out1.txt.gz) --out2 >(gzip > out2.txt.gz)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
